-- a simple tokenizer for our calc language
-- it take a string as input and returns a stream of tokens as output.
-- it uses the Stream structure available from util and demonstrates
-- how regular expression can be used during pattern matching.
-- Lutz Hamel, (c) University of Rhode Island

let test_tokeinizer = false.

load system "type".
load system "stream".

structure Token with
  data type.
  data value.
  end

function get_number
  with input_stream:%Stream do
    let number = "".
    while input_stream @peek() is "[0-9]" do
      let number = number + input_stream @get().
    end
    return tointeger(number).
  end

function tokenize
  with input:%string do
    let input_stream = Stream(input @explode()).
    let output_stream = Stream([]).
    while not input_stream @eof() do
      let c = input_stream @peek().
      if c is "[0-9]" do
        let n = get_number(input_stream).
        let token = Token("number",n).
        output_stream @append(token).
      elif c is "\+" do
        input_stream @get().
        let token = Token("add","+").
        output_stream @append(token).
      elif c is "-" do
        input_stream @get().
        let token = Token("sub","-").
        output_stream @append(token).
      elif c is "\*" do
        input_stream @get().
        let token = Token("mul","*").
        output_stream @append(token).
      elif c is "/" do
        input_stream @get().
        let token = Token("div","/").
        output_stream @append(token).
      elif c is "\(" do
        input_stream @get().
        let token = Token("lparen","(").
        output_stream @append(token).
      elif c is "\)" do
        input_stream @get().
        let token = Token("rparen",")").
        output_stream @append(token).
      elif c is "[\n\t ]" do
        input_stream @get(). -- get and ignore
      else do
        throw Error("unknown symbol: " + c).
      end
    end
    return output_stream.
  end

if test_tokeinizer do
  -- test harness for tokenizer
  load system "io".
  let input_stream = "(101+1)*2".
  println ("tokenizing: "+input_stream).
  let output_stream = tokenize(input_stream).
  println (output_stream @__string__()).
end
